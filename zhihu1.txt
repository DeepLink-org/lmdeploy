代码见： https://github.com/microsoft/chunk-attention

Abstract

自注意力是大型语言模型（LLMs）的一个重要组成部分，但对于长序列来说，它是推理延迟的一个显著来源。在多租户LLMs服务场景中，通过使用多个LLM请求在前缀中共享系统提示的概率，可以优化自注意力的计算和内存操作成本。在本文中，我们介绍了ChunkAttention，这是一个基于前缀的自注意力模块，可以检测多个请求之间匹配的提示前缀，并在运行时共享它们的键/值张量以提高KV缓存的内存利用率。这是通过将整体的键/值张量分解为较小的块并将它们结构化到辅助前缀树中来实现的。因此，在基于前缀树的KV缓存之上，我们设计了一个高效的自注意力核心，其中实现了两阶段分区算法，以提高在存在共享系统提示的情况下进行自注意力计算时的数据局部性。实验证明，与最先进的实现相比，ChunkAttention可以将自注意力核心的速度提高3.2-4.8倍，系统提示的长度范围从1024到4096。


1 引言

在过去几年中，大型语言模型（LLMs）已经发展出各种能力，从上下文学习（Dong等，2023年，2022年）到思维链推理（Chu等，2023年；Wei等，2022年），并在各种自然语言处理相关任务中取得了显著的成功（Chang等，2023年）。代表性的模型有GPT（Radford等，2018年，2019年；Brown等，2020年；OpenAI，2023c）、LLaMA（Touvron等，2023b）、PaLM（Anil等，2023年）和Gemini（Gemini，2023年）系列。随着ChatGPT和GPT store的成功，基于LLM的应用开始激增，并且优化LLM的推理成本的需求已成为一个新的研究兴趣领域（Kim等，2023年；Sheng等，2023年；Aminabadi等，2022年）。



自注意力模块作为LLMs中的一个关键组成部分，在推理过程中性能较差（表1），因为它对上下文标记的键/值张量（KV缓存）进行了密集的内存操作，并且受限于内存（Williams等，2009年；Jin等，2023年）。内存复杂度随上下文长度线性增长。随着对更多上下文标记的需求成为一种趋势（GPT-4为32K），性能变得更差（OpenAI，2023c）。此外，KV缓存还限制了批处理大小和系统吞吐量。例如，使用FP16，在GPT-3（175B）中，每个标记的KV缓存需要4.5MB的内存。具有8个A100（80G）的推理服务器的内存只能容纳70000个标记或35个2K上下文标记的序列。



另一方面，系统提示作为设计LLM基于应用的常见做法，在KV缓存中引入了冗余（Anthropic，2023年）。通常，由于训练和推理成本较高，LLMs以多租户架构进行预训练和部署，以供多个应用程序共享。系统提示对于LLMs来获得每个应用程序的领域知识并生成更好的结果至关重要（White等，2023年；Zhou等，2023年）。由于多个请求共享相同的系统提示，提示前缀中存在显著的重叠（§2.1）。



一个重要的问题是我们是否可以利用系统提示的共享特性来使自注意力模块更快速和更高效地运行。据我们所知，唯一的相关工作是Kwon等人（2023年）提出的一个建议，其中服务提供商为一组预定义的系统提示的键/值张量保留内存，供应用程序开发人员使用。该提议存在一些限制：i）预定义的系统提示在大规模部署中频繁刷新时是静态和不灵活的，因为应用程序开发人员和服务提供商都参与到操作循环中；ii）在系统提示很长且命中率低的情况下会造成内存浪费；iii）在存在共享系统提示的情况下，没有对自注意力核心进行优化的工作。



为了填补这一空白，我们提出了ChunkAttention，这是一个新颖的自注意力模块，具有前缀感知的KV缓存（PAKV）和两阶段分区（TPP）。ChunkAttention中的KV缓存是使用分块上下文标记和键/值张量构建的前缀树。因此，KV缓存是前缀感知的，并且可以在运行时动态检测和删除冗余，无需人为干预。KV缓存仅存储当前正在解码的序列的键/值张量，并且没有内存浪费。此外，前缀树结构为ChunkAttention重新设计高度优化的自注意力核心提供了上下文，其中包括两个阶段的分区：先分块阶段和先序列阶段。具有匹配提示前缀的序列的查询张量被批处理在一起，与键/值张量一起执行注意力。



本文的主要贡献如下：i）我们揭示了系统提示可能很长（§2.1），为优化自注意力提供了机会；ii）我们提出使用前缀树来实现KV缓存，它是开箱即用、可扩展和强大的冗余删除工具；iii）我们实现了一个两阶段分区算法，以加速基于前缀感知的KV缓存上的自注意力核心；iv）我们证明了在各种系统配置下，共享系统提示可以为自注意力带来的性能提升的可行性，并进行了经验量化。



我们的实验证明，与现有的高度优化实现相比，ChunkAttention在共享系统提示长度增加时可以显著提高自注意力核心的速度，并且在没有共享系统提示的情况下没有性能下降。

3 Our Approach

3.1 前缀感知的KV缓存（PAKV）

传统上，KV缓存存储在大小为b × h × n × d的密集张量中，其中b是批量大小，h是头的数量，n是序列长度，d是头维度大小。

当多个序列共享公共前缀标记时，键/值张量相同，因此可以在内存中共享。例如，一个特定的LLM推理服务器首先接收到序列Si = [t1，...，tns，tns+1，...，tnp]，然后接收到序列Sj = [t1，...，tns，t′ns+1，...，t′np]。t1，...，tns的KV缓存只能在内存中有一份物理副本。


鉴于这个特性，我们认为KV缓存应该是前缀感知的，即将所有解码序列的KV缓存组织成前缀树。准确地说，我们沿着序列长度维度在内存中连续地切片单片键/值张量。图1显示了存储在前缀树中的KV缓存的结构。每个节点定义了一个存储三个关键要素的块C：i）由序列Si，...，Sj共享的c个上下文标记片段，以便进行前缀树操作；ii）大小为b×h×c×d的键张量的切片，用于c个标记；iii）相应的值张量切片。前缀树中的每条路径定义了一个序列。服务器中可以同时存在多个树（森林）。例如，应用程序开发人员设计不同的系统提示。

在推理过程中，存在三种可能的情况：i）新序列加入，ii）完成的序列离开，iii）所有序列一起解码一个标记。每种情况都可以转化为前缀树操作。当新序列加入时，搜索并更新前缀树以插入新路径。当完成的序列离开时，更新前缀树以删除其路径。在每次解码迭代中，我们将新标记追加到叶子块中，或者当叶子块已满时增加新块。给定固定的块大小c，内存管理是高效的。在ChunkAttention中，默认采用基于池的内存分配器（Hill，1992；Trebino，2016）。它跟踪一个已使用的块列表和一个空闲的块列表。当请求一个新块时，分配器从空闲列表返回一个块，或者从操作系统（OS）分配新的内存。一旦序列完成，未使用的块将被返回给分配器，但分配器不会释放内存给操作系统，以避免不必要的内存分配。一些用于对齐的内存空间是未使用的。鉴于序列长度为n，内存损失受到(c-1)/n的限制。

通过共享公共前缀，可以将可以同时处理的序列数量增加约1/(1-r)。共享比率r由共享标记数ns/(np + nc)定义，其中nc是完成标记数。在内存受限的推理场景中，这有助于增加批处理大小，从而提高吞吐量。

父子关系定义了每个块所覆盖的序列子集。根节点覆盖所有序列，叶子节点只覆盖一个序列。前缀树的一个关键属性是，在前缀树中每个块所覆盖的序列在序列索引维度上是连续的。因此，在自注意力中切片查询张量特别高效，在下一节中将详细讨论。

3.2 两阶段划分（TPP）

在本节中，我们将深入探讨基于唯一的前缀感知的KV缓存存储的自注意力内核实现。

在预填充期间，我们执行前缀查找，以避免对匹配的提示前缀重复计算KV投影和位置嵌入。对于不匹配的后缀标记，仍然计算KV投影和位置嵌入，并将键/值张量划分并插入前缀树中。然后，我们在整个键/值张量上应用现有的高度优化的自注意力内核，例如FlashAttention（Dao，2023）。

在迭代解码过程中，自注意力被分为块优先和序列优先两个阶段。这两个阶段关注查询张量的不同切片、KV缓存块，并使用不同的并行化策略。该过程如图2所示。由于头维度总是被划分的，所以在我们的讨论中被省略了，但是在实现中是隐含的。


块优先阶段。在块优先阶段，我们只处理被多个序列共享的块。由于GPU具有比头数（Llama 7B为32）更多的流式多处理器（A100为108），而通过头进行划分会低效利用硬件资源，因此我们对键/值进行额外的划分。块划分已经提供了方便。采用在线softmax算法以避免分区之间的同步要求（Milakov和Gimelshein，2018；Dao，2023）。

计算是通过遍历前缀树中的共享块，在执行部分注意力内核partial_attn并将部分注意力结果保存到内存中的过程中进行的，如算法1所示。序列数（批量大小）用b表示。Q ∈ Rb×d是通过连接最新解码迭代中所有b个序列的最后一个标记形成的查询。


partial_attn的实现由公式（1）给出。它独立地计算每个块C的部分注意力结果(O, m, n)(C)，因此可以并行化。Qi:j,:]是Q的一个切片，用于范围从i到j的共享存储在块C中的KV缓存的序列。最大注意力权重向量M(C)是注意力权重W(C)在最后一个维度上的逐行最大值。softmax归一化项n(C)是E(C)在最后一个维度上的逐行求和。M(C)和n(C)是引入的辅助变量，用于进一步累积多个块的部分注意力结果。


局部注意力可以高效地访问共享的KV缓存内存，因为多个序列的自注意力是批量处理的。批处理是在查询序列Si，..., Sj和共享的K(C)/V(C)之间的点积粒度上进行的。除了改善数据局部性之外，批处理的另一个优点是将查询从向量转换为矩阵，从而可以使用张量核心进行高效的矩阵乘法（Choquette等人，2021）。

序列优先阶段。在序列优先阶段，我们从块优先阶段加载共享块的部分注意力结果，并继续处理与一个特定序列相关的块。我们通过切片Q的第i行来划分序列，每个由序列优先内核处理的q都是一个向量，如算法2所示。


attn_reduce重复地将由partial_attn生成的一个块(o, m, n)(C)的部分注意力结果合并到累积注意力结果(o, m, n)中，分别通过缩放它们与x(C)和y(C)。方程(2)展示了这个过程。Oi：，mi和ni是索引i的序列切片。最终的注意力输出是O/n逐元素计算得到的。序列优先阶段在并发性方面非常高效，因为partial_attn和attn_reduce在本地执行，无需线程块之间的通信。然而，由于缺乏由块优先阶段生成的部分注意力结果，它需要从RAM中加载共享块b次，这会增加显著的MOPs。这个两阶段的划分算法平衡了并行化和缓存局部性。


3.3 进一步优化

前缀树结构存储在CPU内存中。为了在GPU上运行两阶段分区内核，我们必须从前缀树中生成某些上下文，包括块C、其覆盖序列的起始索引i和结束索引j，并将上下文(C, i, j)从CPU复制到GPU内存中。例如，在图2中，我们需要生成并复制(C0/C1/C2, 0, 2)、(C3, 0, 0)、(C4/C6, 1, 1)和(C5/C7, 2, 2)。ChunkAttention通过两种方式管理开销：i) 隐藏延迟。在CPU上的上下文生成步骤可以与自注意力之前的其他GPU内核重叠执行。ii) 延迟上下文复制。前缀树在每次解码迭代时不会改变。我们可以将上下文缓存在GPU内存中，并且仅在树结构更改时触发内存复制。触发器包括每个c次迭代的块满、新序列加入和完成的序列离开。开销是分摊的。



在块优先阶段为部分注意力结果分配的临时内存可以通过在partial_attn之后直接执行attn_reduce来消除。由于前缀树中具有父子关系的多个共享块写入相同的(O, m, n)切片，attn_reduce需要串行化。在GPU设备上，原子操作很重，我们不使用这种方法。然而，在CPU设备上，串行化的开销微不足道，可以使用自旋锁来实现减少。



4 实验

评估分别在自注意力微内核级别和端到端GPT-style模型级别进行。微内核级别的评估仅捕捉在自注意力CUDA内核中花费的时间。PAKV和TPP的副作用，如前缀树操作，将在端到端评估中捕捉到。我们使用NVIDIA A100 GPU（80G）和CUDA 11.8运行所有实验。



4.1 微内核评估

基准。我们选择四个自注意力实现作为基准：Naive PyTorch实现（使用公式softmax(QKT /√d)V）、在xformers中实现的内存高效自注意力（Lefaudeux等，2022年）、集成在PyTorch中的FlashAttention（Dao等，2022年）和vLLM中的PagedAttention（Kwon等，2023年）。由于Naive、xformers和FlashAttn都建立在单体KV张量上，它们无法通过部分共享提示前缀的KV缓存来实现前缀感知。PagedAttn也没有实现PAKV。然而，它的分页设计使我们能够手动创建一个固定的页表，将虚拟非共享内存映射到相同的物理内存。它模拟了KV缓存共享的场景，并帮助我们观察PagedAttn的CUDA内核的性能，表示为PagedAttn*。这些内核都不支持TPP算法。



工作负载。以批处理模式处理序列，批处理大小为b。同一批次中的所有序列同时开始和结束。每个序列都预先填充了np个提示标记，前ns个标记是公共前缀。任务是迭代解码下一个nc个完成标记。我们测量解码延迟t和吞吐量（以标记速率表示，每秒标记数或TPS，nc * b / t）。对于所有实验，头部维度d为128，头部数量h为32，块大小c为64。所有张量都为FP16。



结果。我们通过变化以下系统超参数来运行实验，观察PAKV和TPP带来的性能提升：提示和共享标记数、完成标记数和批处理大小。


表3显示了给定不同提示和共享标记数的自注意力实现的延迟。ChunkAttn和PagedAttn*优于Naive、xformers、FlashAttn和PagedAttn，它们对共享标记数不敏感。Naive比ChunkAttn慢6.6倍，比PagedAttn*慢2.1倍（ns=4096）。通过比较PagedAttn*和PagedAttn，我们观察到通过物理共享KV缓存内存带来的性能提升。尽管PagedAttn*没有实现PAKV或TPP，但硬件缓存有助于将其延迟降低高达52%，与PagedAttn（ns=4096）相比：重复访问相同的物理内存块可以提供显著的性能提升。通过比较PagedAttn*和ChunkAttn，可以进一步看到TPP的好处。在ns从1024到4096的范围内，ChunkAttn比PagedAttn*快2.8-3.2倍。当没有共享标记时（ns=0，表3中的ChunkAttn与PagedAttn*比较），TPP不会导致性能回归。因此，应始终启用TPP。随着解码的进行，序列开始分散，ChunkAttn的性能提升逐渐减少，如图3所示。在给定2048个共享标记的情况下，当nc达到512时，ChunkAttn相对于PagedAttn的令牌速率提高了3.6倍，当nc达到2048时，这个速度提升降至2.3倍。然而，这仍然是一个显著的改进。与PagedAttn*相比，ChunkAttn的改进较低，因为PagedAttn*受益于物理共享的KV缓存内存，只有TPP在这里有所区别。然而，给定ns=2048，当nc达到512和2048时，ChunkAttn仍然比PagedAttn*快2.0倍（145K对73K）和1.5倍（70K对46K）。


图4重点关注批处理大小的变化。除了ChunkAttn和PagedAttn*之外，所有实现的吞吐量在批处理大小达到16时达到峰值，这是由于内存限制。鉴于ns为2048，当批处理大小从16增加到96时，ChunkAttn的吞吐量从155k增长到224k toks/s，这是由于更好的数据局部性和改进的算术强度。


4.2端到端评估

ChunkLlama是在Huggingface Llama和vLLM的优化内核（层归一化和旋转嵌入）的基础上构建的，根据Apache-2.0许可证，但是注意模块被ChunkAttn替换。我们在Open Llama2 7B模型上以FP16（Geng和Liu，2023；计算机，2023；Touvron等，2023a）运行所有实验。



基准。我们选择了两个广泛使用和优化的LLM服务工具包，已经证明在生产中使用：最先进的vLLM 0.2.7（Kwon等，2023）和Huggingface的文本生成推理（TGI）1.3.4（HuggingFace，2023）。



工作负载。请求以泊松到达过程（Hill，1992）的方式随机到达服务器，参数化为λ，即平均每秒请求数（RPS）。每个系统在解码期间动态调整实际批处理大小，并将其最大值配置为32个。应用程序开发人员不提供有关服务提供商预配置的共享提示前缀的信息。我们测量规范化延迟（ms/tok或1/TPS），如vLLM中所示，即每个请求的端到端延迟t（包括排队时间）除以完成标记计数nc的平均值，并且KV缓存使用的峰值内存字节。


结果。如图5所示，ChunkLlama具有最快的推理速度。当共享1024和2048个前缀标记时，ChunkLlama可以实现比vLLM高1.6倍（2.9对1.8）和2.3倍（2.3对1.0）的吞吐量，同时保持规范化延迟低于40ms/token。表4比较了我们的ChunkLlama与vLLM的延迟和KV缓存内存使用情况。在没有共享前缀标记的情况下，没有观察到性能回归。长共享前缀的情况下，KV缓存内存使用量减少了70%-90%。由于ChunkLlama可以更快地解码，批处理大小也减少了20%-40%。


5 相关工作

关于优化KV缓存的内存利用的最相关工作是vLLM中的PagedAttention（Kwon等，2023）。它引入了分页技术来解决解码期间由于动态和未知序列长度而导致的内存浪费问题。然而，只提到了服务提供商预配置共享提示的建议，并且在vLLM发布的版本中没有实现（直到0.2.7）。我们的解决方案与分页不同，它使用前缀树来管理内存，并旨在在运行时自动发现KV缓存中用户请求之间的冗余。该解决方案对于服务提供商集中托管模型并具有可扩展性要求的多租户部署场景更实用。根据vLLM，共享的KV缓存类似于多个进程共享的动态链接库。vLLM的策略是在发布之前进行编译（AoT）。我们期望实时编译（JIT）。基于前缀树中捕获的上下文，我们的工作进一步提出了一个两阶段分区算法，以探索共享系统提示对自注意力内核带来的优化机会，这是我们的工作与现有工作之间的另一个区别。



ChunkAttention中的分区策略是基于在线softmax（Milakov和Gimelshein，2018）构建的，并受到FlashAttention（Dao等，2022；Dao，2023）的启发，后者采用了相同的算法。FlashAttention对各种切片技术进行了深入研究和实现，将自注意力加速2-4倍，同时减少内存操作10-20倍。FlashAttention-2改变了切片策略，并额外提高了速度。然而，FlashAttention在处理非连续内存或可变序列长度时不够灵活，更适合于模型训练而不是推理。在解码过程中，当查询标记计数始终为1时，几乎没有收益。ChunkAttention处理解码期间可变的序列长度，并将几个序列的注意力操作批处理以减少内存操作。因此，我们的工作和FlashAttention是互补的。



6 结论

在本文中，我们提出了ChunkAttention，一种新颖的自注意力模块，用于有效管理KV缓存并加速LLM推理的自注意力内核。我们成功地采用前缀树来创建一个前缀感知的KV缓存。它解决了在运行时检测和删除冗余KV缓存的挑战。我们在不同的配置和不同的级别上评估了ChunkAttention，证明了它的可行性和可以管理的副作用。实验结果表明，与没有共享系统提示的SOTA PagedAttention内核相比，ChunkAttention内核可以实现相当的吞吐量，并且可以在具有1024到4096个标记的共享系统提示的情况下实现3.2-4.8倍的性能提升，使用A100（80G）的前缀感知KV缓存和两阶段分区。



7 限制

系统提示的位置。为了共享内存中的键/值张量，共享的系统提示必须出现在序列的开头。尽管这是许多作品和系统中最常见的做法（Lu等，2023；Qian等，2023；Saad-Falcon等，2023；Zhuang等，2023），但这并不是强制性的。Liu等（2023）揭示了当改变相关信息的位置时，语言模型的性能显著下降，这表明模型在长输入上强大地访问和使用信息。特别是，当模型必须在长输入上使用中间信息时，性能通常最低。因此，当应用程序开发人员在评估或无意中出于性能考虑而不将系统提示放在开头时，整个序列的KV缓存是不同的，而PAKV在这种情况下无法节省内存，尽管它们有大量的公共标记。



微调。除了使用系统提示之外，微调是将领域知识融入LLM的另一种有前途的方法（Houlsby等，2019；Hu等，2023）。由于训练和部署成本高，LLM通常是预训练的，并且为了共享多个应用程序而进行集中托管。对于每个应用程序来说，对模型进行微调和部署私有实例是不划算的。然而，随着硬件和软件环境的发展，微调可能变得更加实用和流行。在这种情况下，我们不再需要为每个应用程序设计长的系统提示，并且系统提示的共享机会减少了。截至目前，我们还没有看到比使用系统提示更有前途和成本效益的微调和托管解决方案。



模型和硬件兼容性。为了实现最佳性能，ChunkAttention使用低级CUDA编程而不是利用cuDNN（oneDNN Contributors，2023）或PyTorch中的高级原语来实现两阶段分区内核。我们为最常见的LLM配置（例如，128个头维度大小）和硬件（例如，NVIDIA A100，GeForce RTX 4090和Intel Xeon CPU）调整其性能。对于其他配置和硬件，我们需要逐案调整和验证性能，这增加了显着的开发成本。我们相信需要社区的努力来推广两阶段分区算法，并使其与更多的模型配置和硬件兼容。
